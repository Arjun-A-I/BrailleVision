# Technohack-Team-Sentinal - DYNAMIC BRAILLE WITH USER INPUT OBJECT DETECTION USING AMERICAN SIGN LANGUAGE AND MAY OTHER ML FEATURES
This project aims to improve the daily lives of audio-visually impaired individuals by creating a navigation and interaction system that uses object recognition technology and dynamic braille to identify objects in their environment. Additionally, the system enables the user to find objects using ASL and guides them towards the object. 

# Specific problems  planning to solve with this idea: <br/>
● Audio-Visually impaired people are always dependent on others in their daily
routine.<br/>
● They find it difficult to understand about an object.<br/>
● Unable to interact with a person who doesn't have any knowledge regarding sign
language.<br/>
● Audio-Visually-impaired individuals find it difficult to find things(objects) of their
need .<br/>
● They are unable to explore the world, while following the warnings,signals,etc.<br/>
● Find difficult to use the public transport system.<br/>


 # Solution plan to implement and solve the above mentioned problems:<br/>
● The interaction between the computer vision and the individual is achieved using
a dynamic braille.<br/>
● This device helps in interacting with the surrounding objects and makes their task
easier.(using multiple machine learning models).<br/>
● This device helps in finding the required object from the surrounding using ASL
communication and the hardware leads them to the object.<br/>
● This device can recognize characters in the real world and transfer them to the
individual.<br/>
● Converts speech to braille (communicating between normal person)<br/>
● Has a screen which can be edited by the individual using sign language which
helps in interacting with the public.<br/>
<br/>
<br/>
# Project Description:
There is main machine learning object detection model running in the background which outputs the object name in the dynamic braille created using servo motor,
(this is cost efficient compared to Rs50000 microfluid braille).
when the person concentrate on an object for more than 5 secs the object description is obtained using another machine learning model with object trained using classes
.The interaction include Speech to braille text when interacted with a person , Reads book content when interacted with a readable materials , Traffic signal and sign recognition when interacted with them.
Also has an unique feature in which a person can use Sign language to tell about an object ot the ML model which is detected from the surrounding using YOLOv5 model and their location is passed on to the individual using the braille reader.
# OBJECT DETECTION :
![Screenshot 2023-03-19 144523](https://user-images.githubusercontent.com/98375679/226165654-196b8d23-1a95-4b1b-98df-9a644f5d5a6f.png)
<br/>
<br/>
<br/>
<br/>
# AMERICAN SIGN LAGUAGE TO DETECT OBJECT OF THEIR CHOICE(eg. Apple)
![IMG_8828](https://user-images.githubusercontent.com/98375679/226167902-ff271daa-729c-43d5-949f-423eb85aee68.PNG)

# LIVE TRACKING OF OBJECT USING BRAILLE:
![IMG_E8824](https://user-images.githubusercontent.com/98375679/226167549-54f3945b-001f-44a1-aab5-79a2489193fc.JPG)
<br/>

![IMG_E8826](https://user-images.githubusercontent.com/98375679/226167652-300db26e-16ca-4c8b-9e35-168b0fd42e83.JPG)
<br/>
<br/>
# WHEN PERSON IS DETECTED SPEECH RECOGNITION MODEL COVERTS TO TEXT AND DISPLAYED IN BRAILLE

![IMG_E8830](https://user-images.githubusercontent.com/98375679/226168353-3b893654-25d6-4857-99af-8af1e3009f8a.JPG)

<br/>
# THEIR VISUAL ALONG WITH LIVE LOCATION CAN BE TRACKED:

![WhatsApp Image 2023-03-19 at 3 38 10 PM](https://user-images.githubusercontent.com/98375679/226168956-235bc8cf-343c-4f56-9f8b-b7e204c771b2.jpeg)

